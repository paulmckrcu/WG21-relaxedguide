\documentclass[10]{article}

% standard packages

% A more pleasant font
\usepackage[T1]{fontenc} % use postscript type 1 fonts
\usepackage{textcomp} % use symbols in TS1 encoding
\usepackage{mathptmx,helvet,courier} % use nice, standard fonts for roman, sans and monospace respectively

% Improves the text layout
\usepackage{microtype}

\usepackage{lscape}
\usepackage{fancyhdr}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{url}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{ifthen}
\usepackage{float}
\usepackage{listings}
\lstset{basicstyle=\ttfamily}
% \usepackage[strings]{underscore}
% \usepackage{underscore}
\usepackage[bookmarks=true,bookmarksnumbered=true,pdfborder={0 0 0}]{hyperref}

\lstset{
  literate={\_}{}{0\discretionary{\_}{}{\_}}%
}

\usepackage[table]{xcolor}
\usepackage{booktabs}

\DeclareUrlCommand\email{}

\pagestyle{fancy}
\rhead{}

\newfloat{listing}{tbp}{lol}
\floatname{listing}{Listing}

\begin{document}
\title{A Relaxed Guide to \co{memory_order_relaxed}}

\newcommand{\co}[1]{\lstinline[breaklines=yes,breakatwhitespace=yes]{#1}}

\author{
Hans Boehm\\\email{hboehm@google.com} \and
Paul E.~McKenney\\\email{paulmck@kernel.org} \and
The Indefatigible TBD
}
\date{August 19, 2019}
\maketitle{}

\begin{abstract}
	The out-of-thin-air (OOTA) and read-from-untaken-branch (RFUB)
	properties of the specification of \co{memory_order_relaxed}
	have resulted in considerable consternation over the years.
	Although there are no known instances of full-blown OOTA
	behavior, and no known RFUB-induced failures of production code,
	the theoretical possibility of these properties severely
	complicates automated analysis of large C and C++ code bases.
	Thus far, attempts to eliminate OOTA and RFUB properties from
	the memory model have resulted in otherwise needless added
	overheads on weakly ordered systems on the one hand or
	excessive implementation complexity on the other.
	However, \co{memory_order_relaxed} never was intended to be used
	in arbitrary code, but rather as a part of deliberate application
	of specific concurrency designs.
	This paper forms an initial catalog of patterns underlying such
	designs.
\end{abstract}

\section{OOTA and RFUB Background}
\label{sec:OOTA and RFUB Background}

There has been considerable work done on OOTA and
RFUB~\cite{Boehm:2014:OGA:2618128.2618134,PaulEMcKenney2016OOTA,Lahav:2017:RSC:3062341.3062352,Sinclair:2017:CAR:3079856.3080206,HansBoehm2019OOTArevisitedAgain,MarkBatty2019ModularRelaxedDependenciesOOTA}.
This work has taken place over many years, and builds on prior work
in the Java community.\footnote{
	See for example the infamous ``Causality Test Cases''
	(\url{http://www.cs.umd.edu/~pugh/java/memoryModel/unifiedProposal/testcases.html}).}

There has long been hope that additional research effort will identify
a model of OOTA that all can live with, for example, on the part of Paul,
and that everyone would come to appreciate the relative simplicity of
strengthening \co{memory_order_relaxed} to forbid prior reads to be
reordered with later writes, for example, on the part of Hans.
In addition, although there is general agreement that OOTA behaviors
must be forbidden, there is some debate on the need to forbid
RFUB behaviors.
Perhaps agreement on these points will be reached, but in the meantime,
\co{memory_order_relaxed} use is increasing, and thus an increasing need
to identify known-safe usage patterns.
In the best case, these usage patterns might be automatically recognized
in existing code, but at a minimum we hope that this list will be
useful to code reviewers.

This paper is a first step toward such a set of patterns.

\section{Relaxed Design Patterns}
\label{sec:Relaxed Design Patterns}

Many of these patterns are taken from Hans's \co{memory-model-design}
posting on September 4, 2018.\footnote{
	Message-ID: \co{<CAMOCf+jchGw6DeE2NyCJA3wfFbNH-WFn59JruZPSWt9_jPW9NQ@mail.gmail.com>}.}
@@@ roadmap @@@

\subsection{Non-Racing Accesses}
\label{sec:Non-Racing Accesses}

Any non-racing access to an atomic object can be a relaxed access.
Because the access is not concurrent with a conflicting access (store
against either store or load), further ordering is unnecessary.\footnote{
	This covers case~\#8 in Hans's September 4, 2019 email.}
In fact, such accesses can in theory be non-atomic.
In environments where atomicity is controlled by the access rather
than the object definition, such accesses are often non-atomic in
practice~\cite{JadeAlglave2019WhoAfraidCompiler}.

\begin{listing}[tbp]
\begin{verbatim}
 1 int x = 0;
 2 int y = 0;
 3
 4 void thread1()
 5 {
 6   if (x)
 7     y = 1;
 8 }
 9
10 void thread2()
11 {
12   if (y)
13     x = 1;
14 }
\end{verbatim}
\caption{Non-Atomic Accesses Sometimes Respect Control Dependencies}
\label{lst:Non-Atomic Accesses Sometimes Respect Control Dependencies}
\end{listing}

However, it is important to note that C++ is required to account only
for those dependencies that lead to non-atomic stores.
For example, given concurrent execution of \co{thread1()} and
\co{thread2()} in
Listing~\ref{lst:Non-Atomic Accesses Sometimes Respect Control Dependencies},
the only permitted outcome results in both \co{x} and \co{y} being
equal to zero.
The reason for this is that any other outcome must be due to a
compiler-created data race, which is forbidden.

\begin{listing}[tbp]
\begin{verbatim}
 1 std::atomic<int> x = 0;
 2 std::atomic<int> y = 0;
 3
 4 void thread1()
 5 {
 6   if (x.load(memory_order_relaxed))
 7     y.store(1, memory_order_relaxed);
 8 }
 9
10 void thread2()
11 {
12   if (y.load(memory_order_relaxed)
13     x.store(1, memory_order_relaxed);
14 }
\end{verbatim}
\caption{Normative Text Does Not Require Atomics to Respect Control Dependencies}
\label{lst:Normative Text Does Not Require Atomics to Respect Control Dependencies}
\end{listing}

In contrast, in the analogous program using C++ atomics
(see Listing~\ref{lst:Normative Text Does Not Require Atomics to Respect Control Dependencies}),
additional behaviors are permitted by the normative language in the
standard, including the one resulting in the final values of both \co{x}
and \co{y} being \co{1}.
The phrase ``normative language'' is important because this code fragment
is considered to be an example of the OOTA behavior that is forbidden
by a non-normative note in that same standard.

In short, although any non-racing access to an atomic object may be
relaxed, the normative portion of the standard counter-intuitively
classifies many access patterns as racy.

% \emph{Hans to supply wording for the weird exceptions where the compiler
% is allowed to apply optimizations to \co{memory_order_relaxed} that it
% is not allowed to apply to non-atomic accesses.
% This seems to mostly involve cases where dependencies must be respected
% only to avoid creating data races, a restriction that applies to
% non-atomic accesses but not to relaxed accesses.}

\subsubsection{Initialization and Cleanup}
\label{sec:Initialization and Cleanup}

Important special cases of this pattern are the single-threaded
initialization and cleanup phases of an otherwise concurrent program.
These use cases are one motivation for the strong ordering guarantees
of thread creation and destruction.
These guarantees permit the single-threaded initialization and cleanup
code to run race free, with no need to consider interference from the
intervening code that runs multithreaded.

\subsubsection{Allocator Caches}
\label{sec:Allocator Caches}

Allocator caches provide per-CPU or per-thread pools of free memory
in order to provide high-performance scalable memory allocation in
the common case~\cite{McKenney93,Bonwick94slab}.
Accesses to these pools are normally single-threaded by design for
reasons of performance and scalability.
However, objects are often allocated for concurrent algorithms,
It may be helpful to list phases of a dynamically allocated object's
typical lifetime in a concurrent context:

\begin{enumerate}
\item	Allocation.
\item	Initialization, including construction.
\item	Use.
	This might include subphases, but given that any such
	subphases are defined by the user, safely transitioning between
	them is the user's responsibility.
	This is usually the only phase that permits concurrent access.
\item	Cleanup, including destruction.
\item	Deallocation.
\end{enumerate}

Note the possibility of memory reuse means that this is a cycle rather
than a sequence.

The key point is that there must be a happens-before edge for each
phase transition.
In the case of the cleanup to deallocation to allocation to initialization
transitions, this happens-before edge is frequently supplied by
sequenced-before, courtesy of the fact that allocator caches cause
all of those transitions to occur within a single thread in the common
case.
However, some sort of happens-before edge is required for each phase
transition regardless of which thread is executing any given phase.

In the common case, the transitions requiring other-thread-visible ordering
are those to and from the ``Use'' phase.
In particular, the complexities of transitioning from the ``Use'' phase
to the ``Cleanup'' phase has inspired safe memory reclamation schemes,
including reference counting,
hazard pointers~\cite{Michael02a,HerlihyLM02,MagedMichael04a},
and RCU~\cite{McKenney98}.

In less-common cases where inter-thread transitions occur between other
phases, proper synchronization must be provided.
For example, the earlier phase might use a release store and
the later phase might use an acquire or consume load.

\subsubsection{One-Way Memory Allocation}
\label{sec:One-Way Memory Allocation}

One-way memory allocation provides fresh memory that is never deallocated,
or that is deallocated using a heavy weight one-sided mechanism, for
example, a stop-the-world deallocation phase.
Note that heavy synchronization must be provided upon both entry to and
exit from the stop-the-world phase.

\begin{listing}[tbp]
\begin{verbatim}
 1 void *heap;
 2
 3 void thread1()
 4 {
 5   r1 = x.load(memory_order_relaxed);
 6   y.store(r1, memory_order_relaxed);
 7 }
 8
 9 void thread2()
10 {
11   bool allocated(false);
12   r1 = y.load(memory_order_relaxed);
13   if (r1 != heap) {
14     allocated = true;
15     r1 = heap;
16   }
17   x.store(r1, memory_order_relaxed);
18   assert_not(allocated);
19 }
\end{verbatim}
\caption{RFUB Allocator-Like Example}
\label{lst:RFUB Allocator-Like Example}
\end{listing}

As in
Section~\ref{sec:Allocator Caches},
allocation and initialization is often performed single-threaded,
with the allocated objects passing through the same phases.
And as in that section, a happens-before edge is required for each
transition in each dynamically allocated object's lifetime.
This requirement rules out the infamous RFUB cycle shown in
Listing~\ref{lst:RFUB Allocator-Like Example}.\footnote{
	Adapted from Boehm and
	Demsky\cite[Figure 5]{Boehm:2014:OGA:2618128.2618134}.}
This is because the allocation phase on line~15 is required to happen
before any later phase, a requirement that is violated by the relaxed
accesses on lines~5, 12, and~17.

Code reviewers should therefore view relaxed stores of pointers to
newly allocated objects with great suspicion.

\subsection{Single-Location Data Structures}
\label{sec:Single-Location Data Structures}

Relaxed atomic operations provide sequentially consistent access to
a single object.
This means that data structures that fit into a single object can
be accessed with relaxed atomics with no possibility of OOTA or
RFUB behavior.

Note well that a group of single-location data structures might well
interact in a way that could raise the spectre of OOTA or RFUB.
As before, design review should therefore pay careful attention to
information flow.

\subsection{Shared Fences}
\label{sec:Shared Fences}

The \co{atomic_thread_fence()} function can be used to order
multiple sets of accesses, for example, by replacing a series of
acquire loads with relaxed loads followed by an
\co{atomic_thread_fence(memory_order_acquire)}~\cite[Section 4.1]{RaulSilvera2007WeakMemoryModel}
or a series of release stores with an
\co{atomic_thread_fence(memory_order_release)} followed by
a series of relaxed stores~\cite[Section 4.2]{RaulSilvera2007WeakMemoryModel}.
In many cases, other ordered atomic operations may be substituted for
the fence operations.

In this design pattern, OOTA and RFUB behaviors are ruled out by the semantics
of \co{atomic_thread_fence()}.

\subsection{Atomic Reference-Count Updates}
\label{sec:Atomic Reference-Count Updates}

In certain reference-count use cases, the ordering of the increments and
decrements is irrelevant.
One common case is where it is only legal to increment the reference
count when you already have a reference, in which case the count cannot
possibly decrease to zero in the meantime.
Because only the one-to-zero transition requires ordering, reference-count
increments can be relaxed in cases where another reference is guaranteed
to be held throughout.

@@@ This feels like an example of a more general class of patterns,
but other examples of such a class do not immediately come to mind. @@@

\subsection{Untrusted Loads}
\label{sec:Untrusted Loads}

In many cases, it is acceptable for a load from an atomic shared variable
to occasionally return random bits because the value is checked by
some later operation.
In such cases, the load can be a relaxed load.

\subsubsection{Pre-Load for Compare and Swap}
\label{sec:Pre-Load for Compare and Swap}

Perhaps the most well-known later checking operation is a non-relaxed
compare-and-swap (CAS).
The \co{atomic_compare_exchange_*()} family of read-modify-write
CAS operations are typically used in a loop, and often require an initial
load prior to the first pass through that loop.
For non-relaxed CAS operations, this initial load can typically be a
relaxed load, with the CAS operation's ordering preventing OOTA and RFUB
behaviors.
Relaxed CAS operations need to be part of some other design pattern
(for example, the shared fences pattern called out in
Section~\ref{sec:Shared Fences})
if cycles containing them are to be guaranteed to be OOTA/RFUB-free in
conjunction with an initial relaxed load.
One common design pattern is the single-location data structure discussed in
Section~\ref{sec:Single-Location Data Structures}.

Additional examples are presented by
Sinclair et al.~\cite{Sinclair:2017:CAR:3079856.3080206}.

\subsubsection{Sequence Locking}
\label{sec:Sequence Locking}

The accesses within a sequence-locking read-side critical section
can used relaxed loads because any concurrency with the corresponding
update will result in a retry, thus discarding any loaded values.
Assuming that sequence-locking readers never store to shared memory,
this not only prevents the surfacing of any OOTA or RFUB cycles, but
also of any other non-SC behaviors.

Note that a proposal\footnote{
	\url{http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1478r0.html}}
provides an \co{atomic_load_per_byte_memcpy()} that allows safe
non-atomic access to data for sequence-lock readers, as well as an
\co{atomic_store_per_byte_memcpy()} to update that same data by
sequence-lock updaters.
It is nevertheless quite possible that some sequence-lock readers
might continue to use relaxed atomics in order to permit reliable
computations within readers in the presence of data objects having
trap representations.
Furthermore, more-complex sequence-locking use cases may need to
use relaxed accesses for other
reasons~\cite{NeilBrown2015PathnameLookup,NeilBrown2015RCUwalk}.

\subsection{Unidirectional Data Flow}
\label{sec:Unidirectional Data Flow}

If data flows only in one direction, then OOTA cycles cannot form.
The following sections give several examples of this general design
pattern.

\subsubsection{Independent Input Data}
\label{sec:Independent Input Data}

Input data consisting of independent objects may be read using relaxed
accesses because these objects are not affected by downstream computations.
Here input data is defined broadly, including:

\begin{enumerate}
\item	Measurements of outside environmental conditions.
\item	Device configuration data.
\item	Software configuration data.
\item	Security policies.
\item	Network routing information.
\end{enumerate}

The key point is that the concurrent-computation portion of application
references but does not modify this data, and that there are no
object-to-object consistency constraints.

\subsubsection{Independent Output Data}
\label{sec:Independent Output Data}

Similarly, output data consisting of independent objects may be written
using relaxed accesses because these objects do not affect upstream
computations.
As before, output data is defined broadly, including:

\begin{enumerate}
\item	Control of objects external to the computer.
\item	Many classes of debug output.
\item	Some use cases involving video frame buffers.
\item	Some use cases involving communication to a later stage of a
	software pipeline.
\end{enumerate}

Similar to the independent input data discussed in the preceding section,
the key point is that the concurrent-computation portion of application
modifies but does not reference this data, and that there are no
object-to-object consistency constraints.

\subsubsection{Statistical Counters}
\label{sec:Statistical Counters}

The canonical instance of a unidirectional data-flow pattern is the
statistical counter, in which each thread (or CPU, as the case may be)
updates its own counter, and the aggregate value of the counter is read
out by summing all threads'
counters~\cite[Section 5.2]{McKenney2018ParallelProgramming-2018-12-08a}.

Statistical counters do have concurrent updates and reads, and thus must
use atomics.
However, the concurrent reads can be modeled as returning approximate
results (for example, for monitoring or debugging), and can in fact be
modeled as sequentially consistent approximate operations.
But more to the point, data flow in real use cases is always
unidirectional, proceeding from the updater responding to an event
and flowing through the counter to some reader displaying or logging
statistics.
This unidirectional data flow precludes the cycles required for OOTA or
RFUB behavior to manifest.

\begin{listing}[tbp]
\begin{verbatim}
 1 StatCounter<int> a;
 2 StatCounter<int> b;
 3
 4 void thread1()
 5 {
 6   int r1 = a.readout();
 7   b.increase(r1);
 8 }
 9
10 void thread2()
11 {
12   int r2 = b.readout();
13   a.increase(r2);
14 }
\end{verbatim}
\caption{Statistical-Counter Abuse and OOTA}
\label{lst:Statistical-Counter Abuse and OOTA}
\end{listing}

An example abuse is shown in
Listing~\ref{lst:Statistical-Counter Abuse and OOTA}.
Lines~1 and~2 define a pair of statistical counters \co{a} and \co{b}.
The \co{thread1()} and co{thread2()} functions form a classic
data-dependent OOTA cycle.
Assuming both statistical counters start out with all counters zero,
we could in theory see the following OOTA sequence of events:

\begin{enumerate}
\item	Line~6 sums \co{a}'s counters, obtaining the sum 42.
\item	Line~7 increases the current component of \co{b}'s counter by 42.
\item	Line~12 sums \co{b}'s counters, obtaining the sum 42 due to the
	increase from line~7.
\item	Line~13 increases the current component of \co{a}'s counter by 42,
	thus justifying the sum of 42 obtained by line~6.
\end{enumerate}

Of course, the code in
Listing~\ref{lst:Statistical-Counter Abuse and OOTA}
is complete nonsense: Counters should count events, not each others's
cumulative values.
The code as written is about as useful as the proverbial screen door in
a submarine.
Problems of this sort should be located in a code review, or better yet
during the preceding design review.\footnote{
	Yes, this could be considered analogous to a difference-equation
	control system.
	But in that case, the system being controlled is part of the
	loop, and proper synchronization must be used when communicating
	with that system.
	In addition, the actual difference-equation computation will
	normally be single-threaded.
	More importantly, if the system being controlled might pose a threat
	to life and limb, the design review had jolly well better be
	sufficiently well-informed and thorough as to avoid this sort
	of problem.}

Exact values are sometimes obtained from statistical counters in
stop-the-world situations, such as checking for consistent results
at the end of a stress test or benchmarking
run~\cite[Sections 5.3 and 5.4]{McKenney2018ParallelProgramming-2018-12-08a}.
Alternatively, counter updates might be carried out while read-holding
a given reader-writer lock and counter reads while write-holding
that same lock.
In all of these cases, OOTA and RFUB behaviors are additionally avoided due
to the fully synchronized nature of the readout.

\subsubsection{Owner Field for Re-Entrant Mutex}
\label{sec:Owner Field for Re-Entrant Mutex}

\emph{Note:} There is some debate as to the validity of this design pattern.

A re-entrant exclusive mutex must track its owner in order to avoid
self-deadlock when the owner re-acquires a mutex that it already holds.
This owner field is updated only while the mutex is held, and its value
is used only to compare for equality to the current thread's ID.
Before releasing the mutex, the owner writes a special ID to the owner
field that is guaranteed not to match the ID of any thread.
Other threads can access the owner concurrently with the owner's
update, so the owner field must be atomic in order to avoid data races.

However, the only time that the owner field can be equal to the thread ID
is when that thread carried out the last update to the owner field and
still holds the mutex:

\begin{enumerate}
\item	Each thread writes only its ID or the special ID.
\item	Because \co{memory_order_relaxed} loads are single-variable
	SC, and because each thread sets the owner field to the special
	ID before releasing the mutex, a given thread cannot see its own
	ID unless it still holds the mutex.
\item	Because atomic accesses forbid load tearing, each load from
	the owner field will return either the special ID or the thread
	ID corresponding to some thread that recently held the mutex.
\item	Therefore, when a thread is not holding the mutex, it is guaranteed
	not to load its own ID from the owner field.
\end{enumerate}

No other thread is allowed to write to the owner field while the mutex
is held, so it is impossible to form the cycles required for OOTA or
RFUB behavior to manifest.
Therefore, both the reads from and the writes to the owner field
may use \co{memory_order_relaxed}.

This is a special case of unidirectional data flow, with the data flowing
from the mutex holder to threads not holding the mutex.
The mutual exclusion provided by the mutex prevents any OOTA or RFUB
cycles from forming.

% More generally, if an object is written only while a given mutex is
% held, all accesses to that object may be relaxed without the possibility
% of OOTA or RFUB behaviors.
% --- If all writes to all objects are protected by a given mutex, maybe.

\subsubsection{Relaxed Consumption}
\label{sec:Relaxed Consumption}

In cases where a full-speed \co{memory_order_consume} is needed on a
weak-memory system and where the developers are willing to live within
strict coding standards~\cite{PaulEMcKenney2014rcu-dereference},
\co{memory_order_relaxed} may be used to head dependency chains.
In many (but not all!) use cases, the data flow is also unidirectional,
proceeding from the thread installing the new object to the threads
consuming it.

Note well that this design pattern is outside of the current standard.

\subsection{Java Hashcode Access}
\label{sec:Java Hashcode Access}

\emph{[ Does this apply to C or C++?
Yes, because this portion of Java is written in C++. ]}

\subsection{Chaotic Relaxation}
\label{sec:Chaotic Relaxation}

There are a number of iterative numerical algorithms for which unsynchronized
access does not slow convergence as much as waits for barrier synchronization.
These algorithms can use relaxed loads and stores to update the numerical
data~\cite{Andrews91textbook}.

In theory, these algorithms are subject to OOTA and RFUB behaviors, however,
in practice, current implementations avoid such behaviors.

\subsection{Garbage Collection}
\label{sec:Garbage Collection}

@@@ TBD, but believed to be subject to OOTA and RFUB. @@@

@@@ Does use of things like \co{sys_membarrier()} alleviate this issue
by providing strong ordering without impeding allocation and traversal
in the common case?  @@@

\section{Attributes of Relaxed Design Patterns}
\label{sec:Attributes of Relaxed Design Patterns}

\begin{table}
\renewcommand*{\arraystretch}{1.2}
\newcommand{\x}{\textcolor{gray!20}{\rule{7pt}{7pt}}}
\newcommand{\rothead}[1]{\begin{picture}(6,70)(0,0)\rotatebox{90}{#1}\end{picture}}
\small
\centering
\begin{tabular}{lccccccc}
	\toprule
	& \rothead{Multiple Threads}
	& \rothead{Concurrent WW}
	& \rothead{Concurrent RW}
	& \rothead{~~~~But Checked}
	& \rothead{~~~~But Discarded}
	& \rothead{~~~~Fungible Values}
	& \rothead{Unordered Cycle}
	\\
%				  MT  CWW   CRW    BC   BD   FV    UC
	\cmidrule(r){1-1} \cmidrule{2-8}
	Non-Racing Accesses (Section~\ref{sec:Non-Racing Accesses})
				&  Y & \x &  \x  & \x & \x & \x &  \x \\
	Single-Location Data Structures (Section~\ref{sec:Single-Location Data Structures})
				&  Y &  Y &   Y  & \x & \x & \x &  \x \\
	Shared Fences (Section~\ref{sec:Shared Fences})
				&  Y &  Y &   Y  & \x & \x & \x &  \x \\
	Atomic Reference-Count Updates (Section~\ref{sec:Atomic Reference-Count Updates})
				&  Y &  Y &   Y  & \x & \x &  Y &  \x \\
	Untrusted Loads (Section~\ref{sec:Untrusted Loads})
				&  Y & \x &   Y  &  y &  y &  y &  \x \\
	Unidirectional Data Flow (Section~\ref{sec:Unidirectional Data Flow})
				&  Y & \x &   Y  & \x & \x & \x &  \x \\
	Java Hashcode Access (Section~\ref{sec:Java Hashcode Access})
				&  ? &  ? &   ?  &  ? &  ? &  ? &   ? \\
	Chaotic Relaxation (Section~\ref{sec:Chaotic Relaxation})
				&  Y &  Y &   Y  & \x & \x &  Y &   Y \\
	Garbage Collection (Section~\ref{sec:Garbage Collection})
				&  ? &  ? &   ?  &  ? &  ? &  ? &   ? \\
%	Mutex Owner Field	&  Y & \x &   Y  &  Y & \x & \x &  \x \\
%	Statistical Counters	&  Y & \x &   Y  & \x & \x & \x &  \x \\
%	Single-Threaded
%	   Processing Phases	& \x & \x &  \x  & \x & \x & \x &  \x \\
%	One-Way Memory
%	   Allocation		&  Y &  Y &  \x  & \x & \x & \x &  \x \\
%	Allocator Caches	& \x & \x &  \x  & \x & \x & \x &  \x \\
%	Sequence Locking	&  Y & \x &   Y  & \x &  Y & \x &  \x \\
	\bottomrule
\end{tabular}
\caption{Attributes of Categories of Relaxed Design Patterns}
\label{tab:Attributes of Categories of Relaxed Design Patterns}
\end{table}

@@@ Update @@@

Table~\ref{tab:Attributes of Categories of Relaxed Design Patterns}
shows attributes of design patterns.
The attributes are as follows:

\begin{enumerate}
\item	\emph{Multiple Threads:}  The design pattern uses multiple threads
	in and of itself.
	Note that ostensibly single-threaded patterns often interact
	with other patterns extending across multiple threads.
	For example, allocator caches operate within a single
	thread, but the resulting memory blocks and associated pointers
	will assuredly be passed to other threads using some other
	pattern such as release-acquire or release-consume.
\item	\emph{Concurrent WW:}  The design pattern involves concurrent
	relaxed writes to a given object.
\item	\emph{Concurrent RW:}  The design pattern involves concurrent
	relaxed reads and writes to a given object, but not necessarly
	concurrent relaxed writes.
\item	\emph{But Checked:}  The values from the concurrent reads are
	checked if there is the possibility of a concurrent write.
\item	\emph{But Discarded:}  The values from the concurrent reads are
	discarded if there is the possibility of a concurrent write.
\item	\emph{But Fungible:}  The values from the concurrent reads are
	fungible if there is the possibility of a concurrent write,
	that is, a fixed decision guaranteed to be made based on any
	value from a read that might have run concurrently with a write.
\item	\emph{Unordered Cycle:}  The design pattern can produce an
	unordered cycle in and of itself.
	Of course, a combination of design patterns that individually
	exclude the possibility of an unordered cycle might nevertheless
	produce an unordered cycle when used in combination.
\end{enumerate}

\section{Marking of Relaxed Design Patterns}
\label{sec:Marking of Relaxed Design Patterns}

It is currently believed that these design patterns must be explicitly marked
in order for code reviewers and automatic verifiers to recognize them
and validate their usage.
Here are some candidate marking strategies that have been discussed
within the C++ standards committee:

\begin{enumerate}
\item	Create new \co{memory_order} \co{enum} members for each new
	design pattern.
	This has the benefit of calling out the pattern in an unmistakable
	way that is visible to the compiler, but requires that each
	new design pattern be standardized.
	It also does not support the case where a given access plays a
	role in multiple overlapping design patterns.
\item	Use structured comments to mark each design pattern.
	This avoids the time delays and administrative overhead inherent
	in standardization, and could potentially allow multiple comments
	to handle a given access that plays a role in multiple overlapping
	design patterns.
\item	Use structured comments with a per-instance identifier for
	a given use of a pattern.
	The idea here is to enable tools to more easily spot
	unintended interactions between different design patterns
	being applied to a given group of objects.
	On the other hand, this raises the issue of namespace management.
\item	Define C++ \co{template} types for each design pattern.
	This is an excellent idea where it applies, as it might well
	for the statistical counters discussed in
	Section~\ref{sec:Unidirectional Data Flow}.
	However, we have reason to doubt that \co{template} types can
	be reasonably created for all possible relaxed-access design
	patterns.
\end{enumerate}

More ideation and discussion is needed on this topic.

\section{Use of Relaxed Design Patterns}
\label{sec:Use of Relaxed Design Patterns}

@@@

\section{Concluding Remarks}
\label{sec:Concluding Remarks}

@@@


\bibliographystyle{plain}
\bibliography{bib/RCU,bib/WFS,bib/hw,bib/os,bib/parallelsys,bib/patterns,bib/perfmeas,bib/refs,bib/syncrefs,bib/search,bib/swtools,bib/realtime,bib/TM,bib/standards}

\end{document}
